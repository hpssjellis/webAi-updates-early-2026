<script type="module">
    import {
        AutoProcessor,
        MultiModalityCausalLM,
        RawImage,
        env
    } from "https://cdn.jsdelivr.net/npm/@huggingface/transformers@4.0.0-next.4";

    // Standard v4 setup
    env.allowLocalModels = false;

    // DOM refs
    const initBtn = document.getElementById('initBtn');
    const uiBody = document.getElementById('uiBody');
    const fileInput = document.getElementById('fileInput');
    const queryInput = document.getElementById('queryInput');
    const analyzeBtn = document.getElementById('analyzeBtn');
    const resultText = document.getElementById('resultText');
    const resultBox = document.getElementById('resultBox');
    const statusMsg = document.getElementById('statusMsg');
    const progressBar = document.getElementById('progressBar');
    const progressWrap = document.getElementById('progressWrap');

    let model = null;
    let processor = null;

    // --- 1. Load Model (Same as T2I page) ---
    initBtn.addEventListener('click', async () => {
        initBtn.disabled = true;
        statusMsg.textContent = 'Initializing WebGPU...';
        progressWrap.style.display = 'block';

        try {
            const MODEL_ID = "onnx-community/Janus-Pro-1B-ONNX";

            processor = await AutoProcessor.from_pretrained(MODEL_ID);
            model = await MultiModalityCausalLM.from_pretrained(MODEL_ID, {
                device: 'webgpu',
                dtype: {
                    prepare_inputs_embeds: 'q4',
                    language_model: 'q4',
                    lm_head: 'fp16',
                    gen_head: 'fp16',
                    gen_img_embeds: 'fp16',
                    image_decode: 'fp32',
                }
            });

            statusMsg.textContent = 'üü¢ Ready';
            initBtn.style.display = 'none';
            uiBody.style.display = 'block';
            progressWrap.style.display = 'none';
        } catch (err) {
            statusMsg.textContent = '‚ùå Load failed: ' + err.message;
            initBtn.disabled = false;
        }
    });

    // --- 2. Analyze Image (V4 Multimodal logic) ---
    analyzeBtn.addEventListener('click', async () => {
        const file = fileInput.files[0];
        if (!file) return alert("Please select an image");

        analyzeBtn.disabled = true;
        statusMsg.textContent = 'Reading image...';

        try {
            // Convert file to RawImage
            const blobURL = URL.createObjectURL(file);
            const image = await RawImage.fromURL(blobURL);
            URL.revokeObjectURL(blobURL);

            // IMPORTANT: Janus-Pro vision prompt formatting
            const conversation = [
                {
                    role: '<|User|>',
                    content: '<image_placeholder>\n' + (queryInput.value || "Describe this image."),
                    images: [image],
                }
            ];

            statusMsg.textContent = 'Encoding & Thinking...';
            
            // Apply chat template for vision
            const inputs = await processor(conversation);

            const output_ids = await model.generate({
                ...inputs,
                max_new_tokens: 128,
                do_sample: false, // Better for analysis/description
            });

            // Slicing tokens (removing prompt tokens from output)
            const prompt_len = inputs.input_ids.dims.at(-1);
            const new_tokens = output_ids.slice(null, [prompt_len, null]);
            
            const decoded = processor.batch_decode(new_tokens, {
                skip_special_tokens: true,
            });

            resultText.textContent = decoded[0].trim();
            resultBox.style.display = 'block';
            statusMsg.textContent = '‚úÖ Analysis Complete';
        } catch (err) {
            console.error(err);
            statusMsg.textContent = '‚ùå Error: ' + err.message;
        } finally {
            analyzeBtn.disabled = false;
        }
    });
</script>
