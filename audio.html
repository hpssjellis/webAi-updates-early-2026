<body style="font-family: sans-serif; padding: 20px; text-align: center; background: #111; color: white;">
    <h1>Real-Time Transcription (Whisper)</h1>
    <button id="myStartBtn" style="padding: 15px 30px; font-size: 1.2rem; cursor: pointer; border-radius: 8px; border: none; background: #007bff; color: white;">
        Start Listening
    </button>
    <div id="myTranscript" style="margin-top: 30px; border: 1px solid #444; padding: 20px; min-height: 100px; text-align: left; background: #222; border-radius: 8px;">
        Transcribed text will appear here...
    </div>

    <script type="module">
        import { pipeline } from 'https://cdn.jsdelivr.net/npm/@huggingface/transformers@4.0.0-next.4';

        let myTranscriber;
        let myIsRecording = false;

        async function myStartAudio() {
            if (myIsRecording) return;
            myIsRecording = true;
            
            const myStatusElement = document.getElementById('myTranscript');
            myStatusElement.innerText = "Loading Whisper (WebGPU)...";

            try {
                // 1. Initialize the pipeline
                myTranscriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en', { 
                    device: 'webgpu',
                    dtype: 'fp16' 
                });

                // 2. Request Mic Permission
                const myStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                // 3. Create AudioContext (Whisper works best at 16000Hz)
                const myAudioContext = new AudioContext({ sampleRate: 16000 });
                
                // IMPORTANT: AudioContext often starts 'suspended'. We must resume it.
                if (myAudioContext.state === 'suspended') {
                    await myAudioContext.resume();
                }

                const mySource = myAudioContext.createMediaStreamSource(myStream);
                // 4096 buffer size is standard for real-time processing
                const myProcessor = myAudioContext.createScriptProcessor(4096, 1, 1);

                mySource.connect(myProcessor);
                myProcessor.connect(myAudioContext.destination);

                myStatusElement.innerText = "Listening... (Speak now)";

                myProcessor.onaudioprocess = async (myEvent) => {
                    const myAudioData = myEvent.inputBuffer.getChannelData(0);
                    
                    // We clone the data so the processor doesn't clear it before Whisper finishes
                    const myAudioClone = new Float32Array(myAudioData);
                    
                    const myResult = await myTranscriber(myAudioClone);
                    if (myResult && myResult.text) {
                        myStatusElement.innerText = myResult.text;
                    }
                };

            } catch (myErr) {
                myStatusElement.innerText = "Error: " + myErr.message;
                myIsRecording = false;
            }
        }

        // Static link to the function
        document.getElementById('myStartBtn').onclick = myStartAudio;
    </script>
</body>
