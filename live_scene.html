<body style="font-family: sans-serif; padding: 20px; text-align: center; background: #111; color: white;">
    <h1>Live Scene Description</h1>
    <video id="myVideo" autoplay playsinline style="width: 100%; max-width: 500px; border: 2px solid #444; border-radius: 8px;"></video>
    <div id="mySceneText" style="font-size: 1.2rem; margin-top: 20px; color: #5bc0de; min-height: 3em;">
        Initializing scene analyzer...
    </div>

    <script type="module">
        import { pipeline } from 'https://cdn.jsdelivr.net/npm/@huggingface/transformers@4.0.0-next.4';

        let myCaptioner;
        const myVideoElement = document.getElementById('myVideo');
        const myResultDiv = document.getElementById('mySceneText');

        async function mySetupCamera() {
            const myStream = await navigator.mediaDevices.getUserMedia({ video: true });
            myVideoElement.srcObject = myStream;
            return new Promise(resolve => myVideoElement.onloadedmetadata = () => {
                myVideoElement.play();
                resolve();
            });
        }

        async function myDescribeScene() {
            if (myCaptioner) {
                try {
                    // We pass the video element to the image-to-text pipeline
                    const myOutput = await myCaptioner(myVideoElement);
                    
                    if (myOutput && myOutput[0]) {
                        myResultDiv.innerText = myOutput[0].generated_text;
                    }
                } catch (myError) {
                    console.error("Captioning error:", myError);
                }
            }
            // Scene description is heavier than YOLO, so we wait 1 second between updates
            setTimeout(myDescribeScene, 1000);
        }

        async function myInit() {
            try {
                await mySetupCamera();
                myResultDiv.innerText = "Loading Scene Model (WebGPU)...";
                
                // Using vit-gpt2 for fast browser-based captioning
                myCaptioner = await pipeline('image-to-text', 'Xenova/vit-gpt2-image-captioning', { 
                    device: 'webgpu', 
                    dtype: 'q8' 
                });
                
                myResultDiv.innerText = "Watching scene...";
                myDescribeScene();
            } catch (myErr) {
                myResultDiv.innerText = "Error: " + myErr.message;
            }
        }

        // Static link to start
        window.onload = myInit;
    </script>
</body>
