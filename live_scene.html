<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Live AI Scene Description</title>
</head>
<body style="font-family: sans-serif; padding: 20px; text-align: center; background: #080808; color: white;">
    <h1 style="color: #00ffcc;">âœ¦ Live AI Scene Description</h1>

    <div style="position: relative; display: inline-block;">
        <video id="myVideo" autoplay playsinline muted
            style="width: 100%; max-width: 500px; border: 2px solid #333; border-radius: 12px; background: #000;">
        </video>
        <div id="loadingOverlay" style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); color: #888;">
            Starting camera...
        </div>
    </div>

    <canvas id="mySnapCanvas" style="display: none;"></canvas>

    <div id="mySceneText"
        style="font-size: 1.4rem; margin-top: 20px; color: #5bc0de; min-height: 3em; max-width: 600px; margin-left: auto; margin-right: auto; line-height: 1.4;">
        Initializing WebGPU...
    </div>

    <script type="module">
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@huggingface/transformers@4.0.0-next.4';

        env.allowLocalModels = false;

        let myCaptioner;
        let myIsRunning = false;

        const myVideoElement  = document.getElementById('myVideo');
        const myResultDiv     = document.getElementById('mySceneText');
        const loadingOverlay  = document.getElementById('loadingOverlay');
        const mySnapCanvas    = document.getElementById('mySnapCanvas');
        const mySnapCtx       = mySnapCanvas.getContext('2d');

        async function mySetupCamera() {
            try {
                const myStream = await navigator.mediaDevices.getUserMedia({
                    video: { facingMode: "environment", width: 640, height: 480 }
                });
                myVideoElement.srcObject = myStream;
                return new Promise(resolve => {
                    myVideoElement.onloadedmetadata = () => {
                        myVideoElement.play();
                        mySnapCanvas.width  = myVideoElement.videoWidth  || 640;
                        mySnapCanvas.height = myVideoElement.videoHeight || 480;
                        loadingOverlay.style.display = 'none';
                        resolve();
                    };
                });
            } catch (err) {
                throw new Error("Camera access denied or unavailable.");
            }
        }

        async function myDescribeScene() {
            if (!myCaptioner || !myIsRunning) return;

            if (myVideoElement.readyState >= 2) {
                try {
                    mySnapCtx.drawImage(myVideoElement, 0, 0, mySnapCanvas.width, mySnapCanvas.height);

                    const myOutput = await myCaptioner(mySnapCanvas, {
                        max_new_tokens: 25,
                        do_sample: false,
                    });

                    if (myOutput && myOutput[0]?.generated_text) {
                        myResultDiv.innerText = myOutput[0].generated_text;
                    }
                } catch (myError) {
                    console.error("Inference error:", myError);
                }
            }

            setTimeout(myDescribeScene, 500);
        }

        async function myInit() {
            try {
                await mySetupCamera();
                myResultDiv.innerHTML = "Loading scene model (WebGPU)...<br><small style='color:#666'>~400MB on first run</small>";
                console.log("[Scene AI] Loading model...");

                myCaptioner = await pipeline(
                    'image-to-text',
                    'Xenova/vit-gpt2-image-captioning',
                    {
                        device: 'webgpu',
                        dtype: 'fp32',
                        progress_callback: (p) => {
                            if (p.status === 'initiate') {
                                const msg = `Fetching: ${p.file}`;
                                myResultDiv.innerText = msg;
                                console.log(`[Scene AI] ${msg}`);

                            } else if (p.status === 'progress' && p.total) {
                                const pct = Math.round(p.progress);
                                const mb  = (p.loaded / 1024 / 1024).toFixed(1);
                                const tot = (p.total  / 1024 / 1024).toFixed(1);
                                const msg = `â†“ ${p.file}  ${mb} / ${tot} MB  (${pct}%)`;
                                myResultDiv.innerText = msg;
                                console.log(`[Scene AI] ${msg}`);

                            } else if (p.status === 'done') {
                                const msg = `âœ” Loaded: ${p.file}`;
                                myResultDiv.innerText = msg;
                                console.log(`[Scene AI] ${msg}`);

                            } else if (p.status === 'ready') {
                                const msg = 'ðŸŸ¢ Model ready â€” starting inference...';
                                myResultDiv.innerText = msg;
                                console.log(`[Scene AI] ${msg}`);
                            }
                        },
                    }
                );

                console.log("[Scene AI] Pipeline initialised.");
                myResultDiv.innerText = "Watching scene...";
                myIsRunning = true;
                myDescribeScene();

            } catch (myErr) {
                myResultDiv.innerText = "Error: " + myErr.message;
                myResultDiv.style.color = "#ff6666";
                console.error("[Scene AI] Init failed:", myErr);
            }
        }

        window.onload = myInit;
    </script>
</body>
</html>
