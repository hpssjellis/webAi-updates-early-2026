<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Live YOLO Detection</title>
</head>
<body style="background:#111; color:#eee; text-align:center; font-family:sans-serif; padding:20px">

    <h2 style="color:#00ffcc;">Live YOLO Detection (Transformers.js v4)</h2>

    <div style="position:relative; display:inline-block; width:480px; height:360px; background:#000;">
        <video id="myCam" autoplay playsinline muted
               style="width:480px; height:360px; border:2px solid #444; object-fit:cover; display:block;"></video>
        <canvas id="myDraw" width="480" height="360"
                style="position:absolute; left:0; top:0; pointer-events:none;"></canvas>
    </div>

    <br><br>

    <button id="myGoButton"
            onclick="window.myStartApp()"
            style="padding:10px 20px; font-size:1rem; cursor:pointer;
                   background:#00ffcc; border:none; border-radius:4px; font-weight:bold;">
        Start Camera &amp; AI
    </button>

    <div id="myStatus"
         style="margin-top:12px; font-family:monospace; color:#aaa; font-size:0.9rem;">
        Ready ‚Äî click the button to begin.
    </div>

    <!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
    <script type="module">

        import { pipeline, env, RawImage }
            from "https://cdn.jsdelivr.net/npm/@huggingface/transformers@4.0.0-next.4";

        // ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        env.allowLocalModels = false;
        env.useBrowserCache  = true;   // cache model in IndexedDB after first load

        // ‚îÄ‚îÄ DOM refs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        const myCam    = document.getElementById("myCam");
        const myDraw   = document.getElementById("myDraw");
        const myCtx    = myDraw.getContext("2d");
        const myStatus = document.getElementById("myStatus");
        const myBtn    = document.getElementById("myGoButton");

        // ‚îÄ‚îÄ Offscreen canvas ‚Äì used to snapshot video frames ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        // v4 no longer accepts HTMLVideoElement directly; we must pass a
        // canvas snapshot or a RawImage instead.
        const mySnap    = document.createElement("canvas");
        mySnap.width    = 480;
        mySnap.height   = 360;
        // willReadFrequently: true tells the browser we'll call getImageData often ‚Üí faster
        const mySnapCtx = mySnap.getContext("2d", { willReadFrequently: true });

        let myDetector  = null;
        let myIsRunning = false;

        // ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        function setStatus(msg) {
            myStatus.textContent = msg;
        }

        // ‚îÄ‚îÄ Start webcam ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        window.myStartCam = async function () {
            const stream = await navigator.mediaDevices.getUserMedia({
                video: { width: 480, height: 360, facingMode: "environment" }
            });
            myCam.srcObject = stream;
            return new Promise(resolve => { myCam.onloadedmetadata = resolve; });
        };

        // ‚îÄ‚îÄ Detection loop ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        window.myDetectionLoop = async function () {
            if (!myIsRunning) return;

            // 1. Snapshot current video frame onto the offscreen canvas
            mySnapCtx.drawImage(myCam, 0, 0, mySnap.width, mySnap.height);

            // 2. Clear the overlay canvas
            myCtx.clearRect(0, 0, myDraw.width, myDraw.height);

            try {
                // 3. Build a RawImage from the offscreen canvas ‚Äì this is what
                //    v4 expects for vision pipelines (canvas / RawImage / URL).
                const myFrame = await RawImage.fromCanvas(mySnap);

                // 4. Run the detector
                const myResults = await myDetector(myFrame, {
                    threshold:  0.40,
                    percentage: true   // bounding boxes as 0-100 percentages
                });

                // 5. Draw boxes and labels
                myCtx.lineWidth = 2;
                myCtx.font      = "bold 14px Arial";

                // Log the raw output once so we can inspect the coordinate range
                if (myResults.length > 0 && !window._debuggedOnce) {
                    console.log("Raw detection result[0]:", JSON.stringify(myResults[0]));
                    window._debuggedOnce = true;
                }

                myResults.forEach(obj => {
                    let { xmin, ymin, xmax, ymax } = obj.box;

                    // v4 with percentage:true returns 0-100.
                    // But if values are >1 and <100 treat as percentages,
                    // if values are <=1 treat as 0-1 fractions,
                    // if values are >100 treat as raw pixels relative to model input.
                    let scaleX, scaleY;
                    if (xmax <= 1.0 && ymax <= 1.0) {
                        // 0‚Äì1 normalised fractions
                        scaleX = myDraw.width;
                        scaleY = myDraw.height;
                    } else if (xmax <= 100 && ymax <= 100) {
                        // 0‚Äì100 percentages
                        scaleX = myDraw.width  / 100;
                        scaleY = myDraw.height / 100;
                    } else {
                        // Raw pixel coords from model input (yolos-tiny uses 512px input)
                        // Scale to canvas size
                        scaleX = myDraw.width  / xmax;  // approximate ‚Äî log will reveal true range
                        scaleY = myDraw.height / ymax;
                        // Better: use the actual model input dimensions from the first box
                        // We'll use a fixed known value for yolos-tiny: 512x512 typical
                        scaleX = myDraw.width  / 512;
                        scaleY = myDraw.height / 512;
                    }

                    const x = xmin * scaleX;
                    const y = ymin * scaleY;
                    const w = (xmax - xmin) * scaleX;
                    const h = (ymax - ymin) * scaleY;

                    // Box
                    myCtx.strokeStyle = "#00ffcc";
                    myCtx.strokeRect(x, y, w, h);

                    // Label + score
                    const label = `${obj.label} ${Math.round(obj.score * 100)}%`;
                    const tw    = myCtx.measureText(label).width;
                    const ty    = y > 22 ? y - 4 : y + h + 18;

                    // Background pill
                    myCtx.fillStyle = "rgba(0,0,0,0.60)";
                    myCtx.fillRect(x, ty - 15, tw + 8, 19);

                    // Text
                    myCtx.fillStyle = "#00ffcc";
                    myCtx.fillText(label, x + 4, ty);
                });

                setStatus(`üü¢ AI Active ‚Äî ${myResults.length} object(s) detected`);

            } catch (err) {
                console.error("Inference error:", err);
                setStatus("‚ö†Ô∏è Inference error ‚Äî see console");
            }

            // 6. Schedule next frame
            requestAnimationFrame(window.myDetectionLoop);
        };

        // ‚îÄ‚îÄ Entry point ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        window.myStartApp = async function () {
            myBtn.disabled = true;

            setStatus("üì∑ Requesting camera access‚Ä¶");
            try {
                await window.myStartCam();
            } catch (e) {
                setStatus("‚ùå Camera denied: " + e.message);
                myBtn.disabled = false;
                return;
            }

            setStatus("‚¨áÔ∏è Loading model ‚Äî first run ~25 MB, please wait‚Ä¶");

            try {
                // Xenova/yolos-tiny  ‚Äì public, no auth, 80 COCO classes, fast
                // Alternative: "Xenova/detr-resnet-50"  (heavier, ~160 MB)
                myDetector = await pipeline(
                    "object-detection",
                    "Xenova/yolos-tiny",
                    {
                        progress_callback: p => {
                            if (p.status === "progress" && p.total) {
                                const pct = Math.round((p.loaded / p.total) * 100);
                                setStatus(`‚¨áÔ∏è Downloading model: ${pct}%`);
                            }
                        }
                    }
                );
            } catch (e) {
                setStatus("‚ùå Model load failed: " + e.message);
                console.error(e);
                myBtn.disabled = false;
                return;
            }

            myBtn.style.display = "none";
            myIsRunning = true;
            window.myDetectionLoop();
        };

    </script>
</body>
</html>
