<!DOCTYPE html>
<html>
<body style="background:#111; color:#eee; text-align:center; font-family:sans-serif; padding:20px">

    <h2 style="color: #00ffcc;">Live YOLO Detection (v4)</h2>

    <div style="position: relative; display: inline-block; width: 480px; height: 360px; background: #000;">
        <video id="myCam" autoplay playsinline style="width: 480px; height: 360px; border: 2px solid #444; object-fit: cover;"></video>
        <canvas id="myDraw" width="480" height="360" style="position: absolute; left: 0; top: 0; pointer-events: none;"></canvas>
    </div>

    <br><br>
    <button id="myGoButton" onclick="myStartApp()" style="padding: 10px 20px; font-size: 1rem; cursor: pointer; background: #00ffcc; border: none; border-radius: 4px; font-weight: bold;">Start Camera & AI</button>
    <div id="myStatus" style="margin-top: 10px; font-family: monospace; color: #aaa;">Ready to load...</div>

    <script type="module">
        import { pipeline, env } from "https://cdn.jsdelivr.net/npm/@huggingface/transformers@4.0.0-next.4";

        // Environment Config
        env.allowLocalModels = false;
        env.useBrowserCache = true;

        const myCam = document.getElementById("myCam");
        const myDraw = document.getElementById("myDraw");
        const myCtx = myDraw.getContext("2d");
        const myStatus = document.getElementById("myStatus");
        
        let myDetector = null;
        let myIsRunning = false;

        window.myStartCam = async function() {
            const myStream = await navigator.mediaDevices.getUserMedia({ 
                video: { width: 480, height: 360 } 
            });
            myCam.srcObject = myStream;
            return new Promise((resolve) => myCam.onloadedmetadata = resolve);
        };

        window.myDetectionLoop = async function() {
            if (!myIsRunning) return;

            // 1. Clear the canvas for the new frame
            myCtx.clearRect(0, 0, myDraw.width, myDraw.height);

            try {
                // 2. Run detector with percentage coordinates
                const myResults = await myDetector(myCam, {
                    threshold: 0.5,
                    percentage: true
                });

                // 3. Draw boxes
                myCtx.strokeStyle = "#00ffcc";
                myCtx.lineWidth = 3;
                myCtx.fillStyle = "#00ffcc";
                myCtx.font = "16px Bold Arial";

                myResults.forEach(myObj => {
                    const { xmin, ymin, xmax, ymax } = myObj.box;
                    
                    // Convert percentages to pixel values based on canvas size
                    const myX = (xmin / 100) * myDraw.width;
                    const myY = (ymin / 100) * myDraw.height;
                    const myW = ((xmax - xmin) / 100) * myDraw.width;
                    const myH = ((ymax - ymin) / 100) * myDraw.height;

                    myCtx.strokeRect(myX, myY, myW, myH);
                    myCtx.fillText(`${myObj.label}`, myX, myY > 20 ? myY - 5 : myY + 20);
                });
            } catch (myErr) {
                console.error("Inference Error:", myErr);
            }

            // 4. Call next frame
            requestAnimationFrame(myDetectionLoop);
        };

        window.myStartApp = async function() {
            myStatus.textContent = "Requesting Camera...";
            await myStartCam();

            myStatus.textContent = "Loading YOLOv8 model (this may take a moment)...";
            // Note: yolov8n is a great choice for v4
            myDetector = await pipeline("object-detection", "Xenova/yolov8n");
            
            myStatus.textContent = "AI Active";
            myIsRunning = true;
            document.getElementById("myGoButton").style.display = "none";
            myDetectionLoop();
        };
    </script>
</body>
</html>
