<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Llama-3.2 Streaming Chat ‚Äî Transformers.js v4</title>
    <style>
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body { background: #0d0d0d; color: #eee; font-family: 'Segoe UI', sans-serif; display: flex; flex-direction: column; align-items: center; padding: 20px; min-height: 100vh; }
        h2 { color: #00ffcc; font-size: 1.5rem; margin-bottom: 20px; letter-spacing: 1px; }
        #chatContainer { width: 100%; max-width: 700px; height: 60vh; background: #151515; border: 1px solid #333; border-radius: 8px; display: flex; flex-direction: column; overflow: hidden; }
        #chatHistory { flex: 1; overflow-y: auto; padding: 20px; display: flex; flex-direction: column; gap: 15px; }
        .message { max-width: 85%; padding: 12px 16px; border-radius: 12px; font-size: 0.95rem; line-height: 1.5; white-space: pre-wrap; }
        .user-msg { align-self: flex-end; background: #00ffcc; color: #000; border-bottom-right-radius: 2px; }
        .ai-msg { align-self: flex-start; background: #222; color: #eee; border: 1px solid #333; border-bottom-left-radius: 2px; }
        #inputArea { padding: 15px; background: #1a1a1a; border-top: 1px solid #333; display: flex; gap: 10px; }
        #userInput { flex: 1; background: #0d0d0d; border: 1px solid #444; color: #fff; padding: 10px; border-radius: 6px; outline: none; }
        #userInput:focus { border-color: #00ffcc; }
        #myControls { margin-top: 20px; width: 100%; max-width: 700px; display: flex; flex-wrap: wrap; gap: 15px; justify-content: center; align-items: center; }
        button { padding: 10px 20px; font-weight: bold; cursor: pointer; background: #00ffcc; border: none; border-radius: 5px; }
        button:disabled { opacity: 0.4; }
        select { background: #1e1e1e; color: #eee; border: 1px solid #444; padding: 8px; border-radius: 4px; }
        #myStatus { margin-top: 15px; font-family: monospace; font-size: 0.85rem; color: #888; }
        .badge { padding: 2px 8px; border-radius: 10px; font-weight: bold; font-size: 0.75rem; margin-left: 5px; }
        .webgpu { background: #0d2e20; color: #00ffcc; }
        .wasm { background: #2a2a10; color: #ffcc00; }
    </style>
</head>
<body>

    <h2>‚ö° Llama-3.2 Streaming Chat</h2>

    <div id="chatContainer">
        <div id="chatHistory">
            <div class="message ai-msg">Streaming mode active. Select a model to begin!</div>
        </div>
        <div id="inputArea">
            <input type="text" id="userInput" placeholder="Type a message..." disabled>
            <button id="sendBtn" disabled>Send</button>
        </div>
    </div>

    <div id="myControls">
        <select id="modelSel">
            <option value="onnx-community/Llama-3.2-1B-Instruct">Llama-3.2-1B (Recommended)</option>
            <option value="onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX">DeepSeek-R1-Qwen-1.5B</option>
            <option value="HuggingFaceTB/SmolLM2-135M-Instruct">SmolLM2-135M (Ultra Fast)</option>
        </select>
        <button id="initBtn">üöÄ Initialize Model</button>
        <div id="deviceInfo"></div>
    </div>

    <div id="myStatus">Ready. Local browser inference.</div>

    <script type="module">
        // FIX 1: Correct CDN import for v4 ‚Äî namespace is @huggingface/transformers
        import { pipeline, env, TextStreamer } from "https://cdn.jsdelivr.net/npm/@huggingface/transformers@4.0.0-next.4";

        env.allowLocalModels = false;

        let generator = null;
        let chat_history = [];

        const statusEl = document.getElementById('myStatus');
        const historyEl = document.getElementById('chatHistory');
        const inputEl = document.getElementById('userInput');
        const sendBtn = document.getElementById('sendBtn');
        const initBtn = document.getElementById('initBtn');
        const modelSel = document.getElementById('modelSel');

        // FIX 2: Subclass TextStreamer to provide a UI callback.
        // In v4, there is no `callback_function` constructor option.
        // The correct pattern is to extend TextStreamer and override on_finalized_text(),
        // which is called each time a decoded chunk of text is ready.
        class CallbackTextStreamer extends TextStreamer {
            constructor(tokenizer, callback) {
                super(tokenizer, {
                    skip_prompt: true,
                    skip_special_tokens: true,
                });
                this.callback = callback;
            }
            // on_finalized_text is the v3/v4 hook called with each decoded text chunk
            on_finalized_text(text) {
                this.callback(text);
            }
        }

        function appendMessage(role, text) {
            const msgDiv = document.createElement('div');
            msgDiv.className = `message ${role === 'user' ? 'user-msg' : 'ai-msg'}`;
            msgDiv.textContent = text;
            historyEl.appendChild(msgDiv);
            historyEl.scrollTop = historyEl.scrollHeight;
            return msgDiv;
        }

        async function initModel() {
            const modelId = modelSel.value;
            initBtn.disabled = true;
            modelSel.disabled = true;

            let device = "wasm";
            if (navigator.gpu) {
                try {
                    const adapter = await navigator.gpu.requestAdapter();
                    if (adapter) device = "webgpu";
                } catch (e) { console.warn("WebGPU unavailable, falling back to WASM"); }
            }

            statusEl.textContent = `‚¨áÔ∏è Loading ${modelId}...`;

            try {
                generator = await pipeline("text-generation", modelId, {
                    device: device,
                    // FIX 3: Use fp16 for WebGPU (better perf/memory than fp32), q8 for WASM.
                    // fp32 works but is slower and uses more memory on WebGPU.
                    dtype: device === "webgpu" ? "fp16" : "q8",
                    progress_callback: (p) => {
                        // FIX 4: Progress callback shape is { status, loaded, total, progress, file, name }.
                        // `p.status === 'progress'` is correct, but use p.progress (0-100) for a clean %
                        // or keep using p.loaded / p.total for MB display ‚Äî both work.
                        if (p.status === "progress" && p.total) {
                            const loadedMB = Math.round(p.loaded / 1024 / 1024);
                            const totalMB = Math.round(p.total / 1024 / 1024);
                            statusEl.textContent = `‚¨áÔ∏è Downloading ${p.file ?? ''}: ${loadedMB}MB / ${totalMB}MB`;
                        }
                    }
                });

                document.getElementById('deviceInfo').innerHTML =
                    `<span class="badge ${device}">${device.toUpperCase()} ACTIVE</span>`;
                statusEl.textContent = "üü¢ Model Ready!";
                inputEl.disabled = false;
                sendBtn.disabled = false;
                initBtn.style.display = 'none';

            } catch (err) {
                statusEl.textContent = "‚ùå Error: " + err.message;
                console.error(err);
                initBtn.disabled = false;
                modelSel.disabled = false;
            }
        }

        async function sendMessage() {
            const text = inputEl.value.trim();
            if (!text || !generator) return;

            inputEl.value = "";
            appendMessage('user', text);
            chat_history.push({ role: "user", content: text });

            const aiMsgDiv = appendMessage('assistant', "");
            inputEl.disabled = true;
            sendBtn.disabled = true;

            let fullResponse = "";

            try {
                // FIX 5: Use the CallbackTextStreamer subclass instead of passing
                // `callback_function` directly ‚Äî that option does not exist in v4.
                const streamer = new CallbackTextStreamer(generator.tokenizer, (chunk) => {
                    fullResponse += chunk;
                    aiMsgDiv.textContent = fullResponse;
                    historyEl.scrollTop = historyEl.scrollHeight;
                });

                // The pipeline call with chat history array is correct for v4 ‚Äî
                // chat template formatting is handled internally.
                await generator(chat_history, {
                    max_new_tokens: 512,
                    temperature: 0.7,
                    do_sample: true,
                    streamer: streamer,
                });

                // Store the completed assistant turn in history
                chat_history.push({ role: "assistant", content: fullResponse });

            } catch (err) {
                aiMsgDiv.textContent = "‚ùå Error: " + err.message;
                console.error(err);
            } finally {
                inputEl.disabled = false;
                sendBtn.disabled = false;
                inputEl.focus();
            }
        }

        initBtn.onclick = initModel;
        sendBtn.onclick = sendMessage;
        inputEl.addEventListener('keypress', (e) => {
            if (e.key === 'Enter') sendMessage();
        });
    </script>
</body>
</html>
